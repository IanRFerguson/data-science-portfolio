{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc8f123b",
   "metadata": {},
   "source": [
    "# Positionless Basketball\n",
    "\n",
    "-----------\n",
    "\n",
    "This project has several goals:\n",
    "\n",
    "* Identify features to use in PCA algorithm\n",
    "* Decompose data with PCA\n",
    "* Identify similar observations with K-Means clustering\n",
    "* Measure differences between groups (at low- and high-dimensional spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898cc5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define custom palette to use for visualization\n",
    "sns.set_style('white')\n",
    "\n",
    "# Cluster colors\n",
    "my_palette = ['#26547C', '#EF476F', '#FFD166', '#06D6A0', '#2D1E2F']\n",
    "\n",
    "# Position colors\n",
    "position_palette = ['#292F36', '#4ECDC4', '#E6E6E6', '#FF6B6B', '#FFE66D']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1556a4",
   "metadata": {},
   "source": [
    "--------\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "In this section we'll read in the NBA data we scraped, clean it up, and identify any irregularities or multicollinearity issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0eb659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from 2020-2021 NBA season\n",
    "nba = pd.read_csv('./nba-stats-2021.csv').iloc[:, 2:]\n",
    "\n",
    "# Isolate categorical variables (Name, Team, Position)\n",
    "positions_only = nba.iloc[:, :4]\n",
    "\n",
    "# Create an aggregate row per player (for players that were traded in-season)\n",
    "nba = nba.groupby('Player').mean().sort_values(by='PTS', ascending=False).reset_index()\n",
    "\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d0336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up extra rows in positions_only dataframe\n",
    "po = positions_only[positions_only['Tm'] != 'TOT'].groupby('Player').first().reset_index()\n",
    "\n",
    "# Merge with nba dataframe\n",
    "nba = nba.merge(po, on='Player', how='left').drop(columns=['Age_x_y'])\n",
    "\n",
    "# Use list comprehension to clean up extra characters\n",
    "nba.columns = [x.split('_')[0] for x in nba.columns]\n",
    "\n",
    "nba.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a8333",
   "metadata": {},
   "source": [
    "We have a clean dataset with tidy columns!\n",
    "\n",
    "Let's reduce some potentital noise by filtering out players that don't see the floor very often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1b4595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 10th percentile for minutes played\n",
    "cutoff = np.quantile(nba['MP'], 0.10)\n",
    "\n",
    "# Remove players < 10th percentile of minutes played, hopefully reduce noise\n",
    "nba = nba[nba['MP'] >= 6.58].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5515c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing data\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(nba.isnull())\n",
    "plt.title('NBA Stats Missing Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa918d12",
   "metadata": {},
   "source": [
    "With no missing data to report, we can keep moving forward.\n",
    "\n",
    "We want to plot any variables that are exceptionally collinear - that way we can remove them prior to clustering, to boost our signal a bit. First, we'll isolate the quantitative variables in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de067cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate quantitative features\n",
    "quant_only = nba.select_dtypes(include=np.number)\n",
    "\n",
    "quant_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37132177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Heatmap plotting function\n",
    "def plot_heatmap(DF):\n",
    "    \n",
    "    corr = DF.corr()\n",
    "    mask = np.triu(corr)\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    sns.heatmap(corr, mask=mask)\n",
    "    plt.title('NBA Stats Correlation Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48fc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(quant_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ebd777",
   "metadata": {},
   "source": [
    "Predictably, a few variables appear to be very collinear (e.g., field goals made and field goals attempted). We can drop these without issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080f4d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highly correlated columns\n",
    "raw_stats_to_drop = ['FG', 'FGA', '2P', '2PA', '3P', '3PA', 'FT', 'FTA', 'AST']\n",
    "\n",
    "# Drop factors above\n",
    "quant_only = quant_only.drop(columns=raw_stats_to_drop)\n",
    "\n",
    "# Plot a new correlation matrix\n",
    "plot_heatmap(quant_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd135c2",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "## Decomposition\n",
    "\n",
    "In this section, we'll scale all of our quantitative data, decompose it using PCA, and identify similar observations (i.e., players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cbc9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "quant_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a693e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit StandardScaler object to data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(quant_only)\n",
    "\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2809e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA object with two components (for visualization purposes)\n",
    "pca = PCA(n_components = 2)\n",
    "\n",
    "# Fit to data\n",
    "low_dimensional_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Check shape\n",
    "low_dimensional_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6101bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "sns.scatterplot(low_dimensional_data[:,0], low_dimensional_data[:,1], \n",
    "                alpha=0.75, color='#002642', s=75)\n",
    "\n",
    "plt.title('Low-Dimensional NBA Stats')\n",
    "plt.xlabel('Component #1')\n",
    "plt.ylabel('Component #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a96c384",
   "metadata": {},
   "source": [
    "We see spread on both axes, which suggests that each component is explaining some of the total variance\n",
    "\n",
    "Next we'll want to identify the optimal number of clusters for our dataset using the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d69cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_range = range(1,20)\n",
    "sum_of_squares = []\n",
    "\n",
    "for val in test_range:\n",
    "    \n",
    "    # Instantiate and fit n value to KMeans object\n",
    "    temp = KMeans(n_clusters = val)\n",
    "    temp.fit(low_dimensional_data)\n",
    "    \n",
    "    # Add inertia to container\n",
    "    sum_of_squares.append(temp.inertia_)\n",
    "    \n",
    "# Plot results\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.scatterplot(test_range, sum_of_squares, s=75)\n",
    "plt.xticks(test_range)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9147d",
   "metadata": {},
   "source": [
    "We see a change in model fit around **k = 5**, so we'll opt for 5 unique clusters in this analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabda789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit KMeans object to data\n",
    "model = KMeans(n_clusters=5, random_state=101)\n",
    "model.fit(low_dimensional_data)\n",
    "\n",
    "# Predict based on the KMeans object\n",
    "predicted_values = model.predict(low_dimensional_data)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(low_dimensional_data[:, 0], low_dimensional_data[:, 1],\n",
    "                hue = model.labels_, alpha=0.75, s=75, palette=my_palette)\n",
    "plt.title('K-Means Clusters in Low-Dimensional Space')\n",
    "plt.xlabel('Component #1')\n",
    "plt.ylabel('Component #2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786f7515",
   "metadata": {},
   "source": [
    "-------\n",
    "\n",
    "## On the Court\n",
    "\n",
    "In this section we'll project our predicted cluster values up to the high-dimensional dataset to see how clusters relate to performance on the court"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project cluster labels to the high-dimensional data\n",
    "nba['cluster'] = predicted_values\n",
    "\n",
    "nba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfcd2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "sns.histplot(data=nba, x='PTS', hue='cluster', palette=my_palette,\n",
    "            edgecolor=\".1\", linewidth=1)\n",
    "\n",
    "plt.xlabel('Points Per Game')\n",
    "plt.title('Distribution of Points Per Cluster')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8df8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,10))\n",
    "sns.scatterplot(data=nba, x=\"VORP\", y=\"PER\", \n",
    "                hue=\"cluster\", palette=my_palette, s=75, alpha=0.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bfc3cd",
   "metadata": {},
   "source": [
    "Clearly **Cluster 2** represents the best players in the league - players in this cluster score more points, are more efficient, and less replaceable. Let's see how each cluster breaks down in terms of their positional makeup (the crux of this project, in fact!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa78697",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for cluster in range(0,5):\n",
    "    \n",
    "    temp = nba[nba['cluster'] == cluster].reset_index()\n",
    "    \n",
    "    sns.catplot(data=temp, x=\"Pos\", kind=\"count\", \n",
    "                palette=position_palette, order=['PG', 'SG', 'SF', 'PF', 'C'], height=8)\n",
    "    plt.title(f\"Cluster {cluster} Position Distribution\")\n",
    "    plt.show()\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b1317e",
   "metadata": {},
   "source": [
    "Cluster 2, our best players, have a decent spread of positional players. In fact, almost every cluster has a moderate spread of positions. This speaks to our larger point, that basketball greatness and listed position have little relation to one another.\n",
    "\n",
    "Lastly, we'll dig in to a few stats to see how they differ between clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f179e515",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(0,5):\n",
    "    \n",
    "    temp = nba[nba['cluster'] == cluster].reset_index()\n",
    "    \n",
    "    print(f'---- Cluster {cluster}\\n')\n",
    "    \n",
    "    print(f'Points:\\t\\t\\t{temp[\"PTS\"].mean()}')\n",
    "    print(f'Effective FG%:\\t\\t{temp[\"eFG%\"].mean()}')\n",
    "    print(f'Steals:\\t\\t\\t{temp[\"STL\"].mean()}')\n",
    "    print(f'Blocks:\\t\\t\\t{temp[\"BLK\"].mean()}')\n",
    "    print(f'Assists:\\t\\t{temp[\"AST\"].mean()}')\n",
    "    print(f'Total rebounds:\\t\\t{temp[\"TRB\"].mean()}')\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d6fd38",
   "metadata": {},
   "source": [
    "While we've already identified Cluster 2 as our best players, Cluster 0 players seem to be the best role-players. They score consistently, dish out assists, and shoot at a respectable 53.6 effective field goal %\n",
    "\n",
    "----------\n",
    "\n",
    "## Playoffs or Bust\n",
    "\n",
    "In this final section, we'll see how playoff and non-playoff teams are constructed differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f349c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top and bottom two teams from each conference\n",
    "playoff_teams = ['UTA', 'PHO', 'PHI', 'BRK']\n",
    "lottery_teams = ['HOU', 'OKC', 'DET', 'ORL']\n",
    "\n",
    "all_teams = playoff_teams + lottery_teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fa24a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate players from the above teams\n",
    "playoff_or_bust = nba[nba['Tm'].isin(all_teams)].reset_index(drop=True)\n",
    "\n",
    "playoff_or_bust.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d906fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize playoff status\n",
    "def made_the_playoffs(x):\n",
    "    if x in playoff_teams:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "# Create new playoff status variable   \n",
    "playoff_or_bust['playoff-status'] = playoff_or_bust['Tm'].apply(lambda x: made_the_playoffs(x))\n",
    "playoff_or_bust['cluster'] = playoff_or_bust['cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74928af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=playoff_or_bust[playoff_or_bust['playoff-status'] == 1],\n",
    "            x='cluster', kind='count', height=8, order=['0','1','2','3','4'],\n",
    "           palette=position_palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d028d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(data=playoff_or_bust[playoff_or_bust['playoff-status'] == 0],\n",
    "            x='cluster', kind='count', height=8, order=['0','1','2','3','4'],\n",
    "           palette=position_palette)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f67b0b",
   "metadata": {},
   "source": [
    "Predictably, teams that didn't make the playoffs had significantly fewer Cluster 2 players than teams that did. Indeed, the total number of Cluster 0 players (our steadfast #2 players) is greater in the non-playoff pool ... perhaps teams that don't make the playoffs have a #2 player slotted in as a #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29400fc4",
   "metadata": {},
   "source": [
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
